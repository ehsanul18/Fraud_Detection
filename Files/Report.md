## Abstract

The digital age has opened doors to new possibilities. The process of purchasing an item has taken a different route in the past decade or so. However, the rise of new methods brings in new problems. Now-a-days, the likelihood of a person's digital information being stolen is significant. Bank details, credit card information can be obtained by hackers and used for illegal purchases. This calls for the need of fraud detection services to notify consumers of any illegal activity taken place using their identity. The fraud prevention system would result in the saving of a surplus of money of the consumer. Early detection would mean that the proper authorities are notified of the illegal activities as soon as possible. This, in turn, could speed up the process of apprehending those responsible of conducting the illicit activities. In this report, we look into improving the accuracy for fraud detection which would result in more reliable systems and better consumer experience. The dataset used has been taken from Kaggle. It consists of two separate datasets containing transaction information and identity that can be joined via transaction id. Later, machine learning algorithms are used to obtain a model to predict on the test data. 

## Methodology

The goal is to detect whether a certain transaction is fraud or not. The initial exploration of the dataset revealed large number of null values within the transaction and identity. One after another the problems of the datasets were solved. The problems of the transaction dataset were met followed by the identity dataset.

At first, the statistical metrics (mean, standard deviation, quartiles) were observed to identify anomalies if any. Then, the data types present within the datasets were checked. The transaction dataset contained a number of categorical features that will be addressed later. The main issue that needed immediate attention were the null values of the features. At first, it was checked if any of the attributes contained over 100000 null values. This is because columns with such a large number of empty rows would provide no useful information to the machine learning models. As a result, those columns were identified and immediately removed from the dataset. The categorical values of the remaining columns were then encoded. Encoding is necessary because machine learning models have a better time understanding numerical values during the training phase. Another column "P_emaildomain" was removed as there was no meaningful way to encode the values. Also, it had over 90000 null values. Next, the null values for the remaining columns were filled. The null values of the categorical columns were filled with their respective mode values. The null values for the numeric columns were filled with their respective mean values. Hence, preparation of the training transaction data was completed.

Next, the problems of the training identity data needed to be addressed. At first, there was the removal of the columns/features with over 40000 null values. Then, there was the identification of the categorical columns with the training identity dataset. The columns "id_31" and "DeviceInfo" were removed as there was no meaningful way to encode the values. The remaining categorical columns were encoded. Then, with the help of heatmap, two columns were removed as they had high correlation values with "id_15" and "id_29", respectively. The remaining columns were filled with mean and mode for the numerical and categorical columns, respectively. Once the training transaction and identity datasets were joined, there were no viable option to fill the null values for the "id" attributes. Hence, all the columns with over 100000 null values were removed. The final training dataset was completed. 

Lastly, preparation for the testing transaction and training datasets were required. For the testing transaction dataset, the features considered for the training transaction dataset were considered for the testing transaction dataset. Again, the categorical features were encoded and null values for them were filled with their respective mode values. The remaining attributes were filled with their respective mean values. Similarly, the testing identity dataset had the columns that were considered for the training identity dataset. However, a small problem that remained with the testing identity dataset was the mislabeling of the "id" columns. For example, "id_15" was labelled as "id-15". Again, the categorical columns were encoded. The missing or null values were filled with mean and mode wherever necessary. Similar to the training data, large number of null values remained after the joining of the test transaction and identity dataset. As a result, the columns with over 100000 null values were dropped. Thus, the training and testing dataset were ready for the machine learning models. 

A number of machine learning models and scalers were used to find the optimum combination for the best results. The scalers were MinMaxScaler, StandardScaler, MaxAbsScaler, Quantile Transformer, Power Transformer, and RobustScaler. The machine learning models were Random Forest, Bagging with Random Forest, Gaussian Naive Bayes, Gradient Boosting and XGBoost. 

## Result Analysis

The scalers were applied to the dataset with which the model was trained. After using all the scalers, it was seen that Quantile Transformer performed well. It transforms the attributes/features/columns to follow a uniform or normal distribution. This transformation spreads out the most frequent values of the features of the dataset. It also reduces the impact of outliers if any. Then, a number of machine learning models were used of which the Gaussian Naive Bayes was chosen. The other machine learning models were likely to give better results. However, due to the limitations of the local system, the optimum parameters were not able to be selected. Hyperparameter tuning was used. For the gradient boosting classifier, hyperparameter tuning was used. Grid search and randomized search were used. Unfortunately, execution of the code cells and computing were taking a long time. After hours of execution, computations were still taking place with no sign of any results. Gaussian Naive Bayes is a variant of Naive Bayes but follows Gaussian normal distribution while working with continuous data. It is a very simple classification technique with high functionality that allows the calculation of conditional probability. They are mainly found to be very helpful when inputs have high dimensionality which is true for this dataset with over 100 features. The final result obtained from kaggle is 0.780127.


## Conclusion

The final result indicates that there is room for much improvement. This is a step in the right direction to improve fraud detection systems. There are definitely more techniques that can be applied to improve the final result.
